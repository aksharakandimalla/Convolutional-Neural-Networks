{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cats_and_Dogs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPHxQxRYq5De7nVfP0iFbWx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aksharakandimalla/Convolutional-Neural-Networks/blob/master/cats_and_Dogs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEFmHJyhLVpd",
        "colab_type": "text"
      },
      "source": [
        "The first thing we will do is download the set of images. They're stored as a zip file containing 3,000 images, 2,000 of which we'll use for training, and 1,000 for testing. Once they're downloaded, you'll need access to the underlying operating system of the virtual machine on which this Colab runs. This is available in the OS namespace. \n",
        "\n",
        "This code will unzip the cats and dogs data that you just downloaded, into the /tmp directory. In there, the sub-directories will be recreated, because they're stored that were in the zip file. The next thing is for us to set up our directories as variables, so we can point the generators at them. Now, that we have the directories as variables, we can pass them to the os.list here to take the files from those directories, and load them into Python lists.\n",
        "\n",
        " We can see there's a list of file names like cat253.jpg, etc. On the files tab, we can also inspect the file system and in TMP, we can see our cats versus dogs folder, within which we have training and validation sub-directories, which each contain cats and dogs directories. The images, of course, are stored in there. If we then count the images in each directory, we can be sure that we have the right amounts of images. As we can see, we have a thousand of each animal in Training and 500 of each in Tests for total of 3000. Next up, we can visualize some of the data, so we can see how diverse it is. This code would set a matplotlib which is a Python library for drawing graphics. This code will pick up some random cats and dogs and draw them in a grid. We can then see once it's drawn that there's a lot of diversity in these images. There's different colors of animal, there's different location within the picture, and there's even sometimes multiple items in the picture, like the lady here holding a cat. We can then run it again to see some more.\n",
        "\n",
        "This image containing multiple cats is particularly challenging. Okay. Now, let's build our neural network. We'll import TensorFlow, and then we'll define our model. We'll print the summary, and here you can see the output shape of how the image passed through the layers, and gradually reducing in size through the convolution and pooling. Here, is where we compile our model, defining the loss function and the optimizer. Here, is where we set up the two generators, pointing them at the training and validation sub-directories. These contains sub-directories of their own, each with cats and dogs. When I run it, you'll see the printout. It found 2,000 images in two classes, and that's the training, and 1,000 images in two classes, that's the testing. Now, we can do the training. This time instead of using model.fit, we'll again use model.fit generator because our data source is the generators. Now, you can see they're passed in. Now, we'll start the training and we'll watch it progress. It should take 3-3.5 minutes. When it's done, you can see the accuracy is about 73% and it's not bad. It's not great but it's not bad. Okay. So let's take a look at how the model predicts on some images. So I'm going to choose these five images from my hard drive and upload them. Once they're uploaded, you can see that the classifier gives me a prediction on each. So let's now compare. We can see that the first one is obviously wrong but let's now look at each image to see if we can compare the results. This first one is really impressive and as you can see, the dog is in a very small part of the image and there's lots of other features like trees, mountains, skies and lakes, and the dog also has its face turned away, but the classifier still got it right. This one is obviously a dog, but again it's partially obscured. You can't see the body. It's just a white mass and the face while pronounced, is also hidden amongst the fur, but it got it right. This one's a bit more obvious, but it's also a very different looking dog than the previous one. The eyes, nose and mouth are all part of a dark patch, but the neural network still recognize the dog. This one's also very impressive. There are two cats in the image and one is mostly hidden, but that didn't confuse the model that correctly recognized the cat here. Strangely enough though, this is the one that the model got wrong. Maybe it's because we can only see one eye, or maybe the sharply defined branches to the left confused the model. So take a look, think about what you would do to go through this workbooks at this point and have some fun with it. Find some images of cats and dogs and upload them to the notebook and get it to classify them, then see if you can find one that you would expect to be easy that the prediction gets wrong. Once you've done that, see if you can edit that image in a way that gets it to work.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhBKgvDTW7Sd",
        "colab_type": "text"
      },
      "source": [
        "**step 1-  download the test data.**\n",
        "\n",
        "\n",
        "The wget command is a command line utility for downloading files from the Internet. syntax: wget -O <desired_path/filename>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuphyGqWVbvz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "  -O /tmp/cats_and_dogs_filtered.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIGBuZYRYAMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# OS library to use Operating System libraries, giving you access to the file system\n",
        "import os\n",
        "# The zipfile library to unzip the data.\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5hqaTfBbJRm",
        "colab_type": "text"
      },
      "source": [
        "**step 2- define directories and look at file names**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XBvBA1pbISX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "8ae448f0-8d60-4584-c807-108a75e12d24"
      },
      "source": [
        "base_dir = '/tmp/cats_and_dogs_filtered'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "train_cat_dir = os.path.join(train_dir, 'cats')\n",
        "train_dog_dir = os.path.join(train_dir, 'dogs')\n",
        "validation_cat_dir = os.path.join(validation_dir, 'cats')\n",
        "validation_dog_dir = os.path.join(validation_dir, 'dogs')\n",
        "\n",
        "cat_train_fnames = os.listdir(train_cat_dir)\n",
        "print('there are ',len(cat_train_fnames),'cat image files to train and the first ten file names are as follows')\n",
        "print(cat_train_fnames[:10])\n",
        "\n",
        "dog_train_fnames = os.listdir(train_dog_dir)\n",
        "print('\\n there are ',len(dog_train_fnames),'dog image files to train and the first ten file names are as follows')\n",
        "print(dog_train_fnames[:10])\n",
        "\n",
        "cat_validation_fnames = os.listdir(validation_cat_dir)\n",
        "print('\\n there are ',len(cat_validation_fnames),'cat image files to train and the first ten file names are as follows')\n",
        "print(cat_validation_fnames[:10])\n",
        "\n",
        "dog_validation_fnames = os.listdir(validation_dog_dir)\n",
        "print('\\n there are ',len(dog_validation_fnames),'dog image files to train and the first ten file names are as follows')\n",
        "print(dog_validation_fnames[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "there are  1000 cat image files to train and the first ten file names are as follows\n",
            "['cat.355.jpg', 'cat.903.jpg', 'cat.946.jpg', 'cat.411.jpg', 'cat.855.jpg', 'cat.920.jpg', 'cat.921.jpg', 'cat.692.jpg', 'cat.123.jpg', 'cat.678.jpg']\n",
            "\n",
            " there are  1000 dog image files to train and the first ten file names are as follows\n",
            "['dog.337.jpg', 'dog.3.jpg', 'dog.846.jpg', 'dog.979.jpg', 'dog.548.jpg', 'dog.381.jpg', 'dog.309.jpg', 'dog.370.jpg', 'dog.799.jpg', 'dog.267.jpg']\n",
            "\n",
            " there are  500 cat image files to train and the first ten file names are as follows\n",
            "['cat.2073.jpg', 'cat.2275.jpg', 'cat.2430.jpg', 'cat.2391.jpg', 'cat.2196.jpg', 'cat.2367.jpg', 'cat.2361.jpg', 'cat.2108.jpg', 'cat.2320.jpg', 'cat.2268.jpg']\n",
            "\n",
            " there are  500 dog image files to train and the first ten file names are as follows\n",
            "['dog.2447.jpg', 'dog.2065.jpg', 'dog.2124.jpg', 'dog.2449.jpg', 'dog.2016.jpg', 'dog.2087.jpg', 'dog.2236.jpg', 'dog.2210.jpg', 'dog.2116.jpg', 'dog.2060.jpg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjylSKLmGUcT",
        "colab_type": "text"
      },
      "source": [
        "**step 3-Visualise the dataset to understand the understand the data better**\n",
        "\n",
        "\n",
        "1.   Configure matplot parameters\n",
        "2.   Display a batch of 8 cat and 8 dog pictures. You can rerun the cell to see a fresh batch each time\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQKfAYSfHD74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set up matplotlib to work interactively and view plots inline simultaeously.\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "#  Parameters for our graph; we'll output images in a 4x4 configuration\n",
        "nrows = 4\n",
        "ncols = 4\n",
        "\n",
        "# Index for iterating over images\n",
        "pic_index = 0\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aSaU2yGKEXS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up matplotlib fig, and size it to fit 16 pics\n",
        "# plt. gcf() allows you to get a reference to the current figure when using pyplot\n",
        "fig = plt.gcf()\n",
        "\n",
        "fig.set_size_inches(nrows*4,ncols*4) # The set_size_inches(w,h) method is used to set the figure size in inches.\n",
        "\n",
        "pic_index+=8 # this is to show a new set of images ach tim ethe cell is rerun.\n",
        "\n",
        "# creates list of paths to each file/image eg-  /tmp/cats_and_dogs_filtered/train/cats/cat.355.jpg\n",
        "next_8_cat_pictures = [os.path.join(train_cat_dir,fname) for fname in cat_train_fnames[pic_index-8:pic_index] ]\n",
        "next_8_dog_pictures = [os.path.join(train_dog_dir,fname) for fname in dog_train_fnames[pic_index-8:pic_index] ]\n",
        "\n",
        "for i, img_path in enumerate(next_8_cat_pictures+next_8_dog_pictures):\n",
        "  # Set up subplot; subplot indices start at 1\n",
        "  sp = plt.subplot(nrows, ncols, i+1)\n",
        "  sp.axis('Off') # Don't show axes (or gridlines)\n",
        "\n",
        "  img = mpimg.imread(img_path)\n",
        "  plt.imshow(img)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRWNjY2eanyP",
        "colab_type": "text"
      },
      "source": [
        "**step 4- Build a tensorflow model & then print the model Summary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMejNnbSa3pg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "                                    tf.keras.layers.Conv2D(16, (3,3), activation = 'relu', input_shape=(150,150,3)),\n",
        "                                    tf.keras.layers.MaxPooling2D(2,2),\n",
        "                                    tf.keras.layers.Conv2D(32, (3,3), activation = 'relu'),\n",
        "                                    tf.keras.layers.MaxPooling2D(2,2),\n",
        "                                    tf.keras.layers.Conv2D(64, (3,3), activation = 'relu'),\n",
        "                                    tf.keras.layers.MaxPooling2D(2,2),\n",
        "                                    tf.keras.layers.Flatten(),\n",
        "                                    tf.keras.layers.Dense(512, activation = 'relu'),\n",
        "                                    tf.keras.layers.Dense(1, activation = 'sigmoid')  \n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEMYVSHAc44L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fi1GjVcfey-",
        "colab_type": "text"
      },
      "source": [
        "**step 6- compile the model**\n",
        "\n",
        "NOTE: In this case, using the RMSprop optimization algorithm is preferable to stochastic gradient descent (SGD), because RMSprop automates learning-rate tuning for us. (Other optimizers, such as Adam and Adagrad, also automatically adapt the learning rate during training, and would work equally well here.)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eq2syFcfdsD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tf.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(optimizer = RMSprop(lr= 0.001), loss = 'binary_crossentropy', metrics = ['accuracy')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43trULE5cFok",
        "colab_type": "text"
      },
      "source": [
        "**step 7- set up data generators for \"data preprocessing\"**\n",
        "\n",
        "Data generators read pictures in source file ---> convert pictures to float32 tensors ---> feed them (with their labels) to our model.\n",
        "\n",
        "data that goes into the neural network needs to be normailzed to make it more ammenable to processing by the network. We can use the keras.preprocessing.image.ImageDataGenerator for normalizing, using the rescale parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqF1h9aNfC9b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tf.keras.preprocesssing.image import ImageDataGenerator\n",
        "\n",
        "train_datagenerator = ImageDataGenerator(rescale = 1.0/255.0)\n",
        "validation_datagenerator = ImageDataGenerator(rescale = 1.0/255.0)\n",
        "\n",
        "\n",
        "# This ImageDataGenerator class allows you to instantiate generators of augmented image batches (and their labels) via .flow(data, labels) \n",
        "# or .flow_from_directory(directory). These generators can then be used with the Keras model methods that accept data generators as inputs: fit, \n",
        "# evaluate_generator, and predict_generator.\n",
        "\n",
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "train_genertor = train_datagenerator.flow_from_directory(\n",
        "                                              train_dir,\n",
        "                                              batch_size = 20,\n",
        "                                              class_mode = 'binary'\n",
        "                                              target_size = (150,150)\n",
        ")\n",
        "\n",
        "# Flow validation images in batches of 20 using test_datagen generator\n",
        "validation_genertor = validation_datagenerator.flow_from_directory(\n",
        "                                              valodation_dir,\n",
        "                                              batch_size = 20,\n",
        "                                              class_mode = 'binary'\n",
        "                                              target_size = (150,150)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHpWvh-HndRL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " source_l = len(os.listdir(SOURCE))\n",
        "    fnames = os.listdir(SOURCE)\n",
        "    no_of_test_files = int(source_l * split_size)\n",
        "    source_file_names = random.sample(fnames, len(fnames))\n",
        "    paths_of_source_file_names =  [os.path.join(SOURCE,fname) for fname in source_file_names ]\n",
        "    \n",
        "    \n",
        "        \n",
        "    for test_file_name in paths_of_source_file_names[0:no_of_test_files-1]:\n",
        "        if os.path.getsize(test_file_name) > 0:\n",
        "                try:\n",
        "            copyfile(, TRAINING)\n",
        "    for train_file_name in paths_of_source_file_names[no_of_test_files:source_1-1]:\n",
        "        if os.path.getsize(train_file_name) > 0:\n",
        "            copyfile(SOURCE, TESTING) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBSZtqX9KmVr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    for test_file_name in paths_of_source_file_names[0:no_of_test_files-1]:\n",
        "        if os.path.getsize(test_file_name) > 0:\n",
        "            tt = TESTING + test_file_name\n",
        "            copyfile(test_file_name,tt )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FupXUFb-iBgC",
        "colab_type": "text"
      },
      "source": [
        "**step 8- train the model by feeding in images**\n",
        "\n",
        "You'll see 4 values per epoch -- Loss, Accuracy, Validation Loss and Validation Accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhLjeVAXkF0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(\n",
        "                        train_generator,\n",
        "                        validation_data = validation_generator,\n",
        "                        steps_per_epoch = 100,\n",
        "                        epochs = 15,\n",
        "                        validation_steps = 20,\n",
        "                        verbose = 2\n",
        "\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnNKDehzkzU6",
        "colab_type": "text"
      },
      "source": [
        "**step 9- running the model**\n",
        "\n",
        "upload one or more files from your file system to run thrugh the model, giving an indication of it classification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYqsUvB2yQ_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded=files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path='/content/' + fn\n",
        "  img=image.load_img(path, target_size=(150, 150))\n",
        "  \n",
        "  x=image.img_to_array(img)\n",
        "  x=np.expand_dims(x, axis=0)\n",
        "  images = np.vstack([x])\n",
        "  \n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  \n",
        "  print(classes[0])\n",
        "  \n",
        "  if classes[0]>0:\n",
        "    print(fn + \" is a dog\")\n",
        "    \n",
        "  else:\n",
        "    print(fn + \" is a cat\")\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT_HAqNmyr7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from   tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "\n",
        "# Let's define a new Model that will take an image as input, and will output\n",
        "# intermediate representations for all layers in the previous model after\n",
        "# the first.\n",
        "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
        "\n",
        "#visualization_model = Model(img_input, successive_outputs)\n",
        "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n",
        "\n",
        "# Let's prepare a random input image of a cat or dog from the training set.\n",
        "cat_img_files = [os.path.join(train_cats_dir, f) for f in train_cat_fnames]\n",
        "dog_img_files = [os.path.join(train_dogs_dir, f) for f in train_dog_fnames]\n",
        "\n",
        "img_path = random.choice(cat_img_files + dog_img_files)\n",
        "img = load_img(img_path, target_size=(150, 150))  # this is a PIL image\n",
        "\n",
        "x   = img_to_array(img)                           # Numpy array with shape (150, 150, 3)\n",
        "x   = x.reshape((1,) + x.shape)                   # Numpy array with shape (1, 150, 150, 3)\n",
        "\n",
        "# Rescale by 1/255\n",
        "x /= 255.0\n",
        "\n",
        "# Let's run our image through our network, thus obtaining all\n",
        "# intermediate representations for this image.\n",
        "successive_feature_maps = visualization_model.predict(x)\n",
        "\n",
        "# These are the names of the layers, so can have them as part of our plot\n",
        "layer_names = [layer.name for layer in model.layers]\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "# Now let's display our representations\n",
        "# -----------------------------------------------------------------------\n",
        "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
        "  \n",
        "  if len(feature_map.shape) == 4:\n",
        "    \n",
        "    #-------------------------------------------\n",
        "    # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
        "    #-------------------------------------------\n",
        "    n_features = feature_map.shape[-1]  # number of features in the feature map\n",
        "    size       = feature_map.shape[ 1]  # feature map shape (1, size, size, n_features)\n",
        "    \n",
        "    # We will tile our images in this matrix\n",
        "    display_grid = np.zeros((size, size * n_features))\n",
        "    \n",
        "    #-------------------------------------------------\n",
        "    # Postprocess the feature to be visually palatable\n",
        "    #-------------------------------------------------\n",
        "    for i in range(n_features):\n",
        "      x  = feature_map[0, :, :, i]\n",
        "      x -= x.mean()\n",
        "      x /= x.std ()\n",
        "      x *=  64\n",
        "      x += 128\n",
        "      x  = np.clip(x, 0, 255).astype('uint8')\n",
        "      display_grid[:, i * size : (i + 1) * size] = x # Tile each filter into a horizontal grid\n",
        "\n",
        "    #-----------------\n",
        "    # Display the grid\n",
        "    #-----------------\n",
        "\n",
        "    scale = 20. / n_features\n",
        "    plt.figure( figsize=(scale * n_features, scale) )\n",
        "    plt.title ( layer_name )\n",
        "    plt.grid  ( False )\n",
        "    plt.imshow( display_grid, aspect='auto', cmap='viridis' ) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiH_m2vQzCyp",
        "colab_type": "text"
      },
      "source": [
        "As you can see we go from the raw pixels of the images to increasingly abstract and compact representations. The representations downstream start highlighting what the network pays attention to, and they show fewer and fewer features being \"activated\"; most are set to zero. This is called \"sparsity.\" Representation sparsity is a key feature of deep learning.\n",
        "\n",
        "These representations carry increasingly less information about the original pixels of the image, but increasingly refined information about the class of the image. You can think of a convnet (or a deep network in general) as an information distillation pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OC7EWsfSyyUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#-----------------------------------------------------------\n",
        "# Retrieve a list of list results on training and test data\n",
        "# sets for each training epoch\n",
        "#-----------------------------------------------------------\n",
        "acc      = history.history[     'accuracy' ]\n",
        "val_acc  = history.history[ 'val_accuracy' ]\n",
        "loss     = history.history[    'loss' ]\n",
        "val_loss = history.history['val_loss' ]\n",
        "\n",
        "epochs   = range(len(acc)) # Get number of epochs\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation accuracy per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot  ( epochs,     acc )\n",
        "plt.plot  ( epochs, val_acc )\n",
        "plt.title ('Training and validation accuracy')\n",
        "plt.figure()\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation loss per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot  ( epochs,     loss )\n",
        "plt.plot  ( epochs, val_loss )\n",
        "plt.title ('Training and validation loss'   )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFqUKklqy67l",
        "colab_type": "text"
      },
      "source": [
        "As you can see, we are overfitting like it's getting out of fashion. Our training accuracy (in blue) gets close to 100% (!) while our validation accuracy (in green) stalls as 70%. Our validation loss reaches its minimum after only five epochs.\n",
        "\n",
        "Since we have a relatively small number of training examples (2000), overfitting should be our number one concern. Overfitting happens when a model exposed to too few examples learns patterns that do not generalize to new data, i.e. when the model starts using irrelevant features for making predictions. For instance, if you, as a human, only see three images of people who are lumberjacks, and three images of people who are sailors, and among them the only person wearing a cap is a lumberjack, you might start thinking that wearing a cap is a sign of being a lumberjack as opposed to a sailor. You would then make a pretty lousy lumberjack/sailor classifier.\n",
        "\n",
        "Overfitting is the central problem in machine learning: given that we are fitting the parameters of our model to a given dataset, how can we make sure that the representations learned by the model will be applicable to data never seen before? How do we avoid learning things that are specific to the training data?\n",
        "\n",
        "In the next exercise, we'll look at ways to prevent overfitting in the cat vs. dog classification model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qTLrzaJy1_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, signal\n",
        "\n",
        "os.kill(     os.getpid() , \n",
        "         signal.SIGKILL\n",
        "       )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}